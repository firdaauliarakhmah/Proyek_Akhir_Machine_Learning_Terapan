# -*- coding: utf-8 -*-
"""Book Recommendation System - Fida.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AYiLV18E2OtEpY0-8aWxmysZMuJpi-_6

# **Book Recommendation System - Proyek Akhir ML Terapan**
---
By : [Firda Aulia Rakhmah](https://www.dicoding.com/users/firda_aulia_rakhmah)

# **Project Overview**

**Latar Belakang Proyek**

Berkembangnya teknologi yang semakin pesat di era digital saat ini, akses terhadap berbagai jenis informasi semakin mudah, termasuk buku bagi para mereka yang gemar membaca. Sayangnya, dengan jumlah buku yang setiap tahunnya bertambah, seringkali pembaca menghadapi kesulitan dalam pemilihan buku yang susuai dengan minat dan kebutuhan mereka. Semakin banyak buku bukannya semakin memudahkan, malahan membuat pengambilan keputusan menjadi lebih rumit. Dengan memanfaatkan algoritma pembelajaran mesin dan analisis data yang mendalam, sistem rekomendasi buku hadir sebagai solusi untuk mengatasi masalah ini. Pembuatan sistem rekomendasi ini menggunakan metode content based filtering dan collaborative filtering.

<br>


**Tujuan proyek**

Proyek ini bertujuan untuk membangun model rekomendasi berbasis *Machine Learning* yang mampu membantu pembaca (readers) menemukan buku yang sesuai dengan minat, preferensi, dan kebutuhan mereka. Dengan menghadirkan rekomendasi yang lebih personal dan akurat, sistem ini diharapkan dapat meningkatkan pengalaman membaca pengguna. Selain itu, proyek ini juga berkontribusi pada pengembangan dunia literasi dengan mempermudah akses pembaca terhadap buku-buku berkualitas yang mungkin sebelumnya sulit ditemukan.

# **Data loading**
"""

from google.colab import drive
drive.mount('/content/gdrive')

"""## Import library"""

import os
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import BinaryCrossentropy
from tensorflow.keras.metrics import RootMeanSquaredError

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

import seaborn as sns
import matplotlib.pyplot as plt

"""# **Data Understanding**

Data Understanding adalah tahap awal proyek untuk memahami data yang dimiliki. Berikut ini adalah informasi mengenai dataset yang digunakan sebagai bahan penyelesaian proyek.

<br>


| Jenis | Keterangan |
| ------ | ------ |
| Title | Book Recommendation Dataset |
| Source | [Kaggle](https://www.kaggle.com/datasets/arashnic/book-recommendation-dataset/code?datasetId=1004280&sortBy=relevance&language=Python) |
| Maintainer | [MÃ¶bius](https://www.kaggle.com/arashnic) |
| License | CC0: Public Domain |
| Visibility | Publik |
| Tags |Online Communities, Literature, Art, Recommender Systems, Culture and Humanities|
| Usability | 10.00 |

Membaca dataset yang akan digunakan
"""

df_b = pd.read_csv('/content/gdrive/MyDrive/2-Kuliah/Dicoding/Proyek Akhir/dataset/Books.csv')
df_r = pd.read_csv('/content/gdrive/MyDrive/2-Kuliah/Dicoding/Proyek Akhir/dataset/Ratings.csv')
df_u = pd.read_csv('/content/gdrive/MyDrive/2-Kuliah/Dicoding/Proyek Akhir/dataset/Users.csv')

"""## Melihat jumlah atribut pada masing-masing dataframe"""

print(f'Jumlah data ISBN     : {len(df_b["ISBN"].unique())}')
print(f'Jumlah data Judul    : {len(df_b["Book-Title"].unique())}')
print(f'Jumlah data Penulis  : {len(df_b["Book-Author"].unique())}')
print(f'Jumlah data Penerbit : {len(df_b["Publisher"].unique())}')
print(f'Jumlah data Tahun    : {len(df_b["Year-Of-Publication"].unique())}')

print(f'=====' * 9)
print(f'Jumlah data Pembaca              : {len(df_r["User-ID"].unique())}')
print(f'Jumlah data Buku                 : {len(df_r["ISBN"].unique())}')
print(f'Jumlah data Rating yang diterima : {len(df_r)}')

print(f'=====' * 9)
print(f'Jumlah data User : {len(df_u)}')

"""**Penjelasan :**

- **Jumlah data ISBN**: 271.360 ISBN unik, menunjukkan banyaknya buku yang terdaftar.
- **Jumlah data Judul**: 242.135 judul unik, beberapa buku memiliki lebih dari satu edisi.
- **Jumlah data Penulis**: 102.023 penulis unik, menunjukkan banyaknya kontribusi penulis.
- **Jumlah data Penerbit**: 16.808 penerbit berbeda.
- **Jumlah data Tahun**: 202 tahun penerbitan buku tercatat.
- **Jumlah data Pembaca**: 105.283 pembaca unik.
- **Jumlah data Buku (di rating)**: 340.556 buku yang diberi rating.
- **Jumlah data Rating**: 1.149.780 entri rating.

**Jumlah data User**: 278.858 pengguna unik.

## Univariate Exploratory Data Analysis (EDA)

EDA (Exploratory Data Analysis) adalah proses awal dalam analisis data yang bertujuan untuk memahami struktur, pola, dan karakteristik dataset secara mendalam sebelum melanjutkan ke tahap analisis lebih lanjut atau membangun model.

### Dataset Books.csv
"""

df_b

"""**Penjelasan :**

Dari dataframe yang di tampilkan di atas, kita dapat menyimpulkan bahwa pada dataset ini memiliki beberapa atribut (kolom) yang ada didalamnya. Berikut ini adalah penjelasan mengenai atribut dalam dataset Books :

- `ISBN` : Merupakan kode unik berupa 10 atau 13 digit yang digunakan untuk mengidentifikasi buku secara internasional. Setiap buku memiliki ISBN yang berbeda.
- `Book-Title` : Berisi judul buku yang dimasukkan dalam dataset. Judul ini digunakan untuk mengidentifikasi isi atau nama buku.
- `Book-Author` : Nama penulis buku. Bisa berupa satu penulis atau lebih jika buku ditulis oleh beberapa orang.
- `Year-of-Publication` : Tahun di mana buku diterbitkan untuk pertama kalinya. Informasi ini membantu menentukan usia buku dan relevansinya.
- `Publisher` : Nama penerbit yang bertanggung jawab atas publikasi buku tersebut. Penerbit biasanya mengelola produksi, distribusi, dan pemasaran buku.
- `Image-URL-S` : URL untuk gambar sampul buku dengan ukuran kecil. Biasanya digunakan untuk pratinjau cepat atau thumbnail.
- `Image-URL-M` : URL untuk gambar sampul buku dengan ukuran sedang. Cocok untuk tampilan standar pada aplikasi atau website.
- `Image-URL-L` : URL untuk gambar sampul buku dengan ukuran besar. Berguna untuk tampilan detail.
"""

df_b.info()

"""**Penjelasan :**
- `RangeIndex` : Dataset memiliki 271,360 baris, dari indeks 0 hingga 271,359.
- `Data Columns` : Terdapat 8 kolom dalam dataset.
- `Non-Null Count` : Menunjukkan jumlah nilai yang tidak kosong dalam setiap kolom.
- `Dtype` : Menunjukan Tipe data dari setiap kolom.
-`Memory Usage` : Dataset menggunakan sekitar 16.6 MB memori di RAM.

### Dataset Ratings.csv
"""

df_r

"""**Penjelasan :**

Dari dataframe yang di tampilkan di atas, kita dapat menyimpulkan bahwa pada dataset ini memiliki beberapa atribut (kolom) yang ada didalamnya. Berikut ini adalah penjelasan mengenai atribut dalam dataset Ratings :

- `User-ID` : Menunjukkan ID unik yang diberikan kepada setiap pengguna yang memberikan rating untuk buku tertentu. Atribut ini digunakan untuk mengidentifikasi setiap pengguna dalam dataset.
- `ISBN` : Merupakan kode ISBN yang digunakan untuk mengidentifikasi setiap buku secara unik. Setiap ISBN merepresentasikan satu buku yang dapat dinilai oleh pengguna. Atribut ini membantu menghubungkan rating dengan buku yang relevan.
- `Book-Rating` : Merupakan rating yang diberikan oleh pengguna untuk buku tertentu. Nilai rating bervariasi dari 0 hingga 10, di mana nilai 0 kemungkinan menunjukkan buku yang belum dibaca atau tidak dinilai, sementara nilai yang lebih tinggi mencerminkan tingkat kepuasan pengguna terhadap buku tersebut.
"""

df_r.info()

"""**Penjelasan :**
- `RangeIndex` : Dataset memiliki 1,149,780 baris, dari indeks 0 hingga 1,149,779.
- `Data Columns` : TTerdapat 3 kolom dalam dataset.
- `Non-Null Count` : Menunjukkan jumlah nilai yang tidak kosong dalam setiap kolom. Semua kolom memiliki 1,149,780 nilai non-null, yang berarti tidak ada nilai yang hilang (NaN).
- `Dtype` : Menunjukan Tipe data dari setiap kolom.
-`Memory Usage` : Dataset menggunakan sekitar 26.3 MB memori di RAM.
"""

df_r.describe()

"""**Penjelasan :**

Deskripsi statistik untuk *dataframe* `Ratings` dengan atribut `Book-Rating`, yaitu untuk menampilkan karakteristik statistik, seperti rata-rata (`mean`), simpangan baku/standar deviasi (`std`), nilai minimum (`min`), nilai maksimum (`max`), kuartil bawah/Q1 (`25%`), kuartil tengah/Q2/median (`50%`), dan kuartil atas/Q3 (`75%`) dari *rating* pengguna terhadap buku yang sudah pernah dibaca.
"""

df_r['Book-Rating'].describe().apply(lambda x: '%.f' % x)

"""Visualisasi grafik histogram frekuensi sebaran data *rating* pengguna terhadap buku yang sudah pernah dibaca, dimulai dari *rating* terkecil yaitu 1 hingga *rating* terbesar yaitu 10."""

df_r['Book-Rating'].value_counts().sort_index().plot(
    kind    = 'barh',
    color   = ['r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r'],
    title   = 'Jumlah Rating Buku',
    xlabel  = 'Rating',
    ylabel  = 'Jumlah',
    figsize = (14, 5),
    xticks  = (np.arange(0, 720000, 50000))
).grid(linestyle='-.', linewidth=0.5)

"""**Penjelasan :**

Berdasarkan hasil visualisasi grafik histogram "Jumlah Rating Buku", dapat disimpulkan bahwa rating yang paling sering diberikan pada buku yang telah dibaca adalah rating 0, dengan jumlah sekitar lebih dari 700.000. Rating 0 ini dapat menimbulkan bias dan mempengaruhi hasil analisis, sehingga data dengan rating 0 sebaiknya dihapus pada tahap persiapan data.

### Dataset User
"""

df_u

"""**Penjelasan :**

Dari dataframe yang di tampilkan di atas, kita dapat menyimpulkan bahwa pada dataset ini memiliki beberapa atribut (kolom) yang ada didalamnya. Berikut ini adalah penjelasan mengenai atribut dalam dataset Users :

- `User-ID` : Menunjukan ID user
- `Location` : Lokasi dari user
- `Age` : Usia dari user

Dapat dilihat bahwa terdapat nilai `null` atau `NaN` (*Not a Number*) pada kolom/atribut `Age`. Sehingga perlu dilakukan pemrosesan lebih lanjut pada tahap *data preparation*.
"""

df_u.info()

"""**Penjelasan :**
- `RangeIndex` : Dataset memiliki 278,858 baris, dari indeks 0 hingga 278,857.
- `Data Columns` : TTerdapat 3 kolom dalam dataset.
- `Non-Null Count` : Menunjukkan jumlah nilai yang tidak kosong dalam setiap kolom:
- `Dtype` : Menunjukan Tipe data dari setiap kolom.
-`Memory Usage` : Dataset menggunakan sekitar 6.4 MB memori di RAM.
"""

df_u.describe()

"""# **Data Preprocessing**

Data preprocessing adalah proses mempersiapkan data mentah agar siap digunakan untuk analisis lebih lanjut atau pelatihan model machine learning. Data yang diperoleh sering kali tidak dalam bentuk yang ideal untuk digunakan langsung. Oleh karena itu, preprocessing diperlukan untuk membersihkan, mengubah, dan menyusun data agar lebih sesuai dengan kebutuhan analisis atau algoritma yang akan digunakan. Dalam kasus ini, tahap *data preprocessing* dilakukan dengan menyesuaikan nama kolom atau atribut masing-masing *dataframe*, melakukan penggabungkan data ISBN, dan data *User* untuk melihat jumlah data secara keseluruhan.

## Rename nama kolom

Perubahan nama kolom bertujuan untuk memudahkan proses pemanggilan dataframe dengan nama kolom yang lebih mudah diingat.

### Books.csv
"""

df_b.rename(columns={
    'ISBN'                : 'isbn',
    'Book-Title'          : 'book_title',
    'Book-Author'         : 'book_author',
    'Year-Of-Publication' : 'pub_year',
    'Publisher'           : 'publisher',
    'Image-URL-S'         : 'image_s_url',
    'Image-URL-M'         : 'image_m_url',
    'Image-URL-L'         : 'image_l_url'
}, inplace=True)

df_b

"""### Ratings.csv"""

df_r.rename(columns={
    'User-ID'     : 'user_id',
    'ISBN'        : 'isbn',
    'Book-Rating' : 'book_rating'
}, inplace=True)

df_r

"""### Users.csv"""

df_u.rename(columns={
    'User-ID'  : 'user_id',
    'Location' : 'location',
    'Age'      : 'age'
}, inplace=True)

df_u

"""## Penggabungan Data ISBN

Penggabungan data ISBN buku dilakukan dengan menggunakan fungsi `.concatenate` yang disediakan oleh *library* `numpy`. Data ISBN terdapat pada *dataframe* buku (`df_b`) dan *dataframe* *rating* (`df_r`), sehingga kedua data tersebut digabungkan berdasarkan kolom `isbn`.
"""

ISBNAll = np.concatenate((
    df_b.isbn.unique(),
    df_r.isbn.unique()
))

ISBNAll = np.sort(np.unique(ISBNAll))

print(f'Jumlah Buku berdasarkan ISBN : {len(ISBNAll)}')

"""## Penggabungan Data User

Penggabungan data `user_id` buku dilakukan menggunakan fungsi `.concatenate` dengan bantuan *library* `numpy`. Data `user_id` terdapat pada *dataframe* *rating* (`df_r`) dan *dataframe* *user* (`df_u`), sehingga dilakukan penggabungan data tersebut pada atribut atau kolom `user_id`.
"""

UserAll = np.concatenate((
    df_r.user_id.unique(),
    df_u.user_id.unique()
))

UserAll = np.sort(np.unique(UserAll))

print(f'Jumlah Buku berdasarkan ISBN : {len(UserAll)}')

"""# **Data Preparation**

Data Preparation adalah proses mengolah dan mempersiapkan data agar siap digunakan untuk analisis atau pelatihan model machine learning. Proses ini merupakan kelanjutan dari Data Understanding dan fokus utamanya adalah membersihkan, mengubah, dan menyusun data untuk meningkatkan kualitas serta relevansi data dengan kebutuhan analisis. Dalam kasus ini, tahap *data preparation* dilakukan dengan mengatasi *missing value*, pengecekan data duplikat, dan penggabungan data buku dan data *rating*.

## Missing value

Missing value adalah nilai yang hilang atau tidak ada dalam sebuah dataset. Hal ini terjadi ketika data tidak tersedia atau tidak tercatat untuk suatu entri atau atribut tertentu. Missing value sering ditemukan dalam berbagai bentuk, seperti kosong, NaN (Not a Number), atau null, dan bisa muncul karena berbagai alasan, seperti kesalahan pengumpulan data, ketidaksesuaian antara sumber data, atau kelalaian dalam pencatatan. Pengecekan *missing value* pada *dataframe* dapat dilakukan dengan menggunakan fungsi `.isnull().sum()`, yang akan menghasilkan total jumlah data yang kosong atau hilang (*missing*).

### Books.csv
"""

books = df_b
books.isnull().sum()

"""**Penjelasan:**

Berdasarkan deskripsi di atas, dapat dilihat bahwa pada *dataframe* `books` terdapat beberapa atribut yang memiliki nilai kosong atau *null*, yaitu pada kolom `book_author` sebanyak 2 data, `publisher` sebanyak 2 data, dan `image_l_url` sebanyak 3 data.

Oleh karena itu, data yang kosong tersebut dapat dihapus dengan menggunakan fungsi `.dropna()`. Setelah penghapusan, pengecekan ulang akan menunjukkan bahwa tidak ada lagi data yang kosong atau *null*.
"""

books = books.dropna()
books.isnull().sum()

"""### Ratings.csv"""

ratings = df_r
ratings.isnull().sum()

"""**Penjelasan:**

Berdasarkan deskripsi di atas, dapat dilihat bahwa pada *dataframe* `ratings`, tidak ditemukan adanya nilai kosong atau *null* pada setiap kolom atau atributnya.

Namun, pada tahap *Univariate Exploratory Data Analysis* (EDA) sebelumnya, hasil visualisasi grafik histogram "Jumlah Rating Buku" menunjukkan bahwa sebagian besar data *rating* dari buku yang telah dibaca oleh *user* memiliki nilai 0, dengan jumlah lebih dari 700.000. *Rating* 0 ini dapat menyebabkan bias dalam analisis data, sehingga data dengan *rating* 0 perlu dihapus.
"""

print(f'Total Rating 0 : {ratings.book_rating.eq(0).sum()}')

ratings = ratings[df_r.book_rating > 0]

"""**Penjelasan :**

Data dengan *rating* 0 ternyata sebanyak 716.109 data. Data tersebut tidak akan diikutsertakan ke dalam *dataframe*, sehingga data yang diambil adalah data *rating* yang lebih dari 0, yaitu *rating* 1 hingga *rating* 10 saja.
"""

ratings.book_rating.value_counts().sort_index().plot(
    kind    = 'barh',
    color   = ['r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r'],
    title   = 'Jumlah Rating Buku',
    xlabel  = 'Rating',
    ylabel  = 'Jumlah',
    figsize = (14, 5),
    xticks  = (np.arange(0, 110000, 7500))
).grid(linestyle='-.', linewidth=0.5)

"""**Penjelasan :**

Berdasarkan hasil visualisasi grafik histogram di atas dengan *rating* 0 yang telah dihapus, dapat dilihat distribusi frekuensi data yang lebih rapi dan jelas, terutama pada data *rating* 1 hingga *rating* 4.

### Users.csv
"""

users = df_u
users.isnull().sum()

"""**Penjelasan :**

Dapat dilihat bahwa pada *dataframe* `users` terdapat atribut yang memiliki nilai kosong atau *null*, yaitu pada atribut `age` sebanyak 110.762 data.


Dengan begitu, data yang kosong tersebut dapat diganti atau diisi dengan nilai modus atau nilai yang paling sering muncul dalam data `age` tersebut dengan menggunakan fungsi `.fillna()` dan fungsi ` .mode()`.
"""

users.age = users.age.fillna(users.age.mode())
users.isnull().sum()

users.age.hist(bins=100)

"""**Penjelasan :**

Berdasarkan hasil visualisasi grafik histogram umur *user* di atas dapat dilihat bahwa rentang umur *user* paling banyak berada pada umur 20 hingga 30-an.

## Duplicate data

Untuk memeriksa adanya data duplikat atau data yang sama dalam sebuah dataframe, kita dapat menggunakan fungsi .duplicated().sum().
"""

print(f'Jumlah data books  yang duplikat: {books.duplicated().sum()}')
print(f'Jumlah data rating yang duplikat: {ratings.duplicated().sum()}')
print(f'Jumlah data users  yang duplikat: {users.duplicated().sum()}')

"""**Penjelasan :**

Berdasarkan data di atas, dapat dilihat bahwa tidak terdapat data buku, *rating*, dan *user* yang memiliki data duplikat.

## Menggabungkan Data Buku dan Rating
"""

books_ratings = pd.merge(ratings, books, on=['isbn'])
books_ratings

"""#**Modeling**

Tahap selanjutnya adalah proses *modeling* atau membuat model *machine learning* yang dapat digunakan sebagai sistem rekomendasi untuk menentukan rekomendasi buku yang terbaik kepada pengguna dengan beberapa algoritma sistem rekomendasi tertentu.

Berdasarkan tahap pemahaman data sebelumnya, terlihat bahwa jumlah data pada setiap *dataframe*âyaitu data buku, *rating*, dan *users*âtergolong sangat besar, mencapai ratusan ribu hingga jutaan entri. Hal ini dapat berpotensi meningkatkan biaya dan waktu yang diperlukan untuk proses pemodelan *machine learning*, serta membebani penggunaan *resource* seperti RAM atau GPU. Oleh karena itu, untuk efisiensi, dalam kasus ini data yang akan digunakan dalam pemodelan *machine learning* akan dibatasi menjadi 10.000 baris data buku dan 5.000 baris data *rating*.

Proses pemodelan yang akan dilakukan menggunakan content based filtering dan collaborative filtering.
"""

books   = books[:10000]
ratings = ratings[:5000]

"""## Content Based Filtering

Pada tahap inilah mengembangkan sistem rekomendasi dengan teknik content based filtering. Ingatlah, teknik content based filtering akan merekomendasikan item yang mirip dengan item yang disukai pengguna di masa lalu. Pada tahap ini, akan menemukan representasi fitur penting dari setiap kategori masakan dengan tfidf vectorizer dan menghitung tingkat kesamaan dengan cosine similarity.

### TF-IDF Vectorizer

*Term Frequency Inverse Document Frequency Vectorizer* `TF-IDF Vectorizer` *Algorithm* merupakan algoritma yang dapat melakukan kalkulasi dan transformasi dari teks mentah menjadi representasi angka yang memiliki makna tertentu dalam bentuk matriks serta dapat digunakan dan dimengerti oleh model *machine learning*.
"""

tfidf = TfidfVectorizer()
tfidf.fit(books.book_author)

"""Proses transformasi data buku dengan atribut `book_author` ke dalam bentuk matriks dapat dilakukan dengan menggunakan fungsi `.fit_transform()`."""

tfidf_matrix = tfidf.fit_transform(books.book_author)
tfidf_matrix.shape

"""**Penjelasan :**

Ukuran matriks yang dihasilkan dari transformasi tersebut adalah 10.000 data buku dan 5.575 data *author*.


Data di atas masih dalam bentuk vektor (dari *vectorizer*), sehingga perlu diubah ke dalam bentuk matriks dengan menggunakan fungsi `.todense()`.
"""

tfidf_matrix.todense()

"""Untuk menampilkan matriks TF-IDF, matriks tersebut perlu diubah terlebih dahulu menjadi sebuah *dataframe* dengan kolom yang berisi nama *author* dan baris (*index*) yang berisi judul buku."""

# Melihat matriks TF-IDF

pd.DataFrame(
    tfidf_matrix.todense(),
    columns = tfidf.get_feature_names_out(),
    index   = books.book_title
).sample(20, axis=1).sample(10, axis=0)

"""### Cosine Similarity

Untuk melakukan perhitungan derajat kesamaan (*similarity degree*) antar judul buku dapat dilakukan dengan teknik *cosine similarity* menggunakan fungsi `cosine_similarity` dari library `sklearn`.
"""

cosine_sim = cosine_similarity(tfidf_matrix)
cosine_sim

"""Dengan teknik yang sama, untuk melihat *array* *cosine similarity* dapat diubah terlebih dahulu menjadi sebuah *dataframe*."""

cosine_sim_df = pd.DataFrame(
    cosine_sim,
    columns = books.book_title,
    index   = books.book_title
)

print(f'Cosine Similarity Shape : {cosine_sim_df.shape}')

cosine_sim_df.sample(8, axis=1).sample(8, axis=0)

"""### Recommendation testing

Mendefinisikan fungsi `author_recommendations` untuk menampilkan data buku yang direkomendasikan oleh algoritma sistem yang telah dibuat, dengan parameter masukan berupa judul buku yang sudah pernah dibaca oleh *user*.
"""

def author_recommendations(book_title, similarity_data=cosine_sim_df, items=books[['book_title', 'book_author']], k=10):
    index = similarity_data.loc[:,book_title].to_numpy().argpartition(range(-1, -k, -1))
    closest = similarity_data.columns[index[-1:-(k+2):-1]]
    closest = closest.drop(book_title, errors='ignore')

    return pd.DataFrame(closest).merge(items).head(k)

readed_book_title = 'Proxies'

books[books.book_title.eq(readed_book_title)]

"""Pada beberapa kasus, sistem rekomendasi akan memberikan rekomendasi buku yang terduplikat, sehingga perlu dilakukan penghapusan data judul buku rekomendasi yang terduplikat."""

author_recommendations(readed_book_title).drop_duplicates()

"""**Penjelasan :**

Dapat dilihat bahwa sistem yang telah dibangun berhasil memberikan rekomendasi beberapa judul buku berdasarkan input atau masukan sebuah judul buku, yaitu "Proxies", dan diperoleh beberapa judul buku yang berdasarkan perhitungan sistem.

## Collaborative Filtering

Sistem rekomendasi penyaringan kolaboratif (*Collaborative Filtering Recommendation*) adalah teknik yang memberikan rekomendasi item berdasarkan preferensi pengguna di masa lalu, misalnya dengan menggunakan *rating* yang telah diberikan oleh pengguna, serta menyarankan item yang mirip dengan pola preferensi pengguna lainnya.

### Data Preparation
"""

# Melakukan encoding fitur user_id

user_ids = ratings.user_id.unique().tolist()
user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}
user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}

print(user_ids)
print(user_to_user_encoded)
print(user_encoded_to_user)

# Melakukan encoding fitur ISBN

book_ids = ratings.isbn.unique().tolist()
book_to_book_encoded = {x: i for i, x in enumerate(book_ids)}
book_encoded_to_book = {i: x for i, x in enumerate(book_ids)}

print(book_ids)
print(book_to_book_encoded)
print(book_encoded_to_book)

# Memetakan user_id dan isbn kedalam dataframe

ratings['user'] = ratings.user_id.map(user_to_user_encoded)
ratings['book'] = ratings.isbn.map(book_to_book_encoded)

"""Melakukan pengecekan jumlah *user*, jumlah buku, dan *rating* minimal serta *rating* maksimal."""

num_users = len(user_encoded_to_user)
num_books = len(book_encoded_to_book)
print(num_users)
print(num_books)

min_ratings = min(ratings.book_rating)
max_ratings = max(ratings.book_rating)
print(f'Number of User: {num_users}, Number of Books: {num_books}, Min Rating: {min_ratings}, Max Rating: {max_ratings}')

"""### Training data dan validasi data

Melakukan pengecekan terdahap *dataframe* `ratings` yang telah dilakukan pemetaan atribut atau kolom tambahan, yaitu `user` dan `book`. Selain itu, dilakukan juga pengacakan data dengan menggunakan fungsi `.sample(frac=1)`.
"""

ratings = ratings.sample(frac=1, random_state=412)
ratings

"""Melakukan pembagian *dataset* dengan rasio 80:20, yaitu 80% untuk data latih (*training data*) dan 20% untuk data uji (*validation data*)."""

x = ratings[['user', 'book']].values
y = ratings['book_rating'].apply(lambda x: (x-min_ratings) / (max_ratings-min_ratings)).values

train_indices = int(0.8 * ratings.shape[0])

xTrain, xVal, yTrain, yVal = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)

print(x, y)

"""### Model development dan training menggunakan RecommenderNet

Pada tahap pembuatan model akan menggunakan kelas `RecommenderNet` dengan [*keras model class*]
"""

class RecommenderNet(tf.keras.Model):
    def __init__(self, num_users, num_books, embedding_size, **kwargs):
        super(RecommenderNet, self).__init__(**kwargs)
        self.num_users = num_users
        self.num_books = num_books
        self.embedding_size = embedding_size
        self.user_embedding = layers.Embedding(
            num_users,
            embedding_size,
            embeddings_initializer = 'he_normal',
            embeddings_regularizer = keras.regularizers.l2(1e-6)
        )
        self.user_bias      = layers.Embedding(num_users, 1)
        self.book_embedding = layers.Embedding(
            num_books,
            embedding_size,
            embeddings_initializer = 'he_normal',
            embeddings_regularizer = keras.regularizers.l2(1e-6)
        )
        self.book_bias = layers.Embedding(num_books, 1)

    def call(self, inputs):
        user_vector = self.user_embedding(inputs[:,0])
        user_bias   = self.user_bias(inputs[:, 0])
        book_vector = self.book_embedding(inputs[:, 1])
        book_bias   = self.book_bias(inputs[:, 1])

        dot_user_book = tf.tensordot(user_vector, book_vector, 2)

        x = dot_user_book + user_bias + book_bias

        return tf.nn.sigmoid(x)

"""Kemudian pada proses *model compiling*, akan menggunakan `Adam optimizer`, `binary crossentropy loss function`, dan metrik `RMSE` (Root Mean Squared Error)."""

model = RecommenderNet(num_users, num_books, 50)

model.compile(
    optimizer = Adam(learning_rate=0.001),
    loss      = BinaryCrossentropy(),
    metrics   = [RootMeanSquaredError()]
)

# Pelatihan model atau model *training* dengan menggunakan fungsi .fit() dengan parameter batch_size sebesar 20, dan 30 epochs.

history = model.fit(
    x               = xTrain,
    y               = yTrain,
    batch_size      = 20,
    epochs          = 30,
    validation_data = (xVal, yVal),
)

# Melakukan visualisasi hasil *training* dan *validation* *error* serta *training* dan *validation* *loss* ke dalam grafik plot

rmse     = history.history['root_mean_squared_error']
val_rmse = history.history['val_root_mean_squared_error']

loss     = history.history['loss']
val_loss = history.history['val_loss']

plt.figure(figsize = (12, 4))
plt.subplot(1, 2, 1)
plt.plot(rmse,     label='RMSE')
plt.plot(val_rmse, label='Validation RMSE')
plt.title('Training and Validation Error')
plt.xlabel('Epoch')
plt.ylabel('Root Mean Squared Error')
plt.legend(loc='lower right')

plt.subplot(1, 2, 2)
plt.plot(loss,     label='Training Loss')
plt.plot(val_loss, label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend(loc='upper right')

plt.show()

"""### Recommendation Testing"""

datasetBook   = books
datasetRating = ratings

"""Untuk mendapatkan rekomendasi buku dari sistem, kita perlu mengambil sampel data pengguna secara acak dan mendefinisikan variabel yang berisi daftar buku yang belum pernah dibaca oleh pengguna (`notReadedBooks`). Daftar ini akan menjadi buku-buku yang akan direkomendasikan. Untuk memperoleh daftar tersebut, kita dapat menggunakan operator logika bitwise [`~`] pada daftar buku yang sudah dibaca oleh pengguna (`readedBooks`)."""

userId      = datasetRating.user_id.sample(1).iloc[0]
readedBooks = datasetRating[datasetRating.user_id == userId]

notReadedBooks = datasetBook[~datasetBook['isbn'].isin(readedBooks.isbn.values)]['isbn']
notReadedBooks = list(
    set(notReadedBooks).intersection(set(book_to_book_encoded.keys()))
)

notReadedBooks = [[book_to_book_encoded.get(x)] for x in notReadedBooks]
userEncoder    = user_to_user_encoded.get(userId)
userBookArray = np.hstack(
    ([[userEncoder]] * len(notReadedBooks), notReadedBooks)
)

"""Untuk mendapatkan hasil rekomendasi buku yang akan diberikan oleh sistem, dapat menggunakan fungsi `.predict()` dari *library* Keras."""

ratings = model.predict(userBookArray).flatten()

topRatingsIndices   = ratings.argsort()[-10:][::-1]
recommendedBookIds = [
    book_encoded_to_book.get(notReadedBooks[x][0]) for x in topRatingsIndices
]

print('Showing recommendations for users: {}'.format(userId))
print('=====' * 8)
print('Book with high ratings from user')
print('-----' * 8)

topBookUser = (
    readedBooks.sort_values(
        by = 'book_rating',
        ascending=False
    )
    .head(5)
    .isbn.values
)

bookDfRows = datasetBook[datasetBook['isbn'].isin(topBookUser)]
for row in bookDfRows.itertuples():
    print(row.book_title, ':', row.book_author)

print('=====' * 8)
print('Top 10 Books Recommendation')
print('-----' * 8)

recommended_resto = datasetBook[datasetBook['isbn'].isin(recommendedBookIds)]
for row in recommended_resto.itertuples():
    print(row.book_title, ':', row.book_author)

"""**Penjelasan :**

Berdasarkan hasil di atas, dapat dilihat bahwa sistem akan mengambil pengguna secara acak, yaitu pengguna dengan `user_id` **278843**. Selanjutnya, sistem akan mencari buku dengan *rating* tertinggi dari pengguna tersebut, yaitu:

*   **Divine Secrets of the Ya-Ya Sisterhood : A Novel** oleh **Rebecca Wells**  
*   **Icy Sparks** oleh **Gwyn Hyman Rubio**  
*   **The Bonesetter's Daughter** oleh **Amy Tan**  
*   **The Things They Carried** oleh **Tim O'Brien**  

Kemudian, sistem akan membandingkan antara buku dengan *rating* tertinggi dari pengguna tersebut dan semua buku lainnya yang belum pernah dibaca, lalu mengurutkan buku yang akan direkomendasikan berdasarkan nilai prediksi rekomendasi tertinggi. Terdapat 10 daftar buku yang direkomendasikan oleh sistem, yaitu:

*   **To Kill a Mockingbird** oleh **Harper Lee**  
*   **The Secret Life of Bees** oleh **Sue Monk Kidd**  
*   **The Bean Trees** oleh **Barbara Kingsolver**  
*   **Life of Pi** oleh **Yann Martel**  
*   **Chasing the Dime** oleh **Michael Connelly**  
*   **A Walk in the Woods: Rediscovering America on the Appalachian Trail** oleh **Bill Bryson**  
*   **The Cabinet of Curiosities** oleh **Douglas Preston**  
*   **Wuthering Heights** oleh **Emily Bronte**  
*   **The Visitor (Animorphs, No 2)** oleh **K. A. Applegate**  
*   **The King of Torts** oleh **John Grisham**  

Dapat dibandingkan antara daftar ***Book with high ratings from user*** dan ***Top 10 Books Recommendation***, terdapat beberapa kesesuaian pola rekomendasi berdasarkan preferensi pengguna. Hal ini menunjukkan bahwa sistem yang dibangun dapat memberikan rekomendasi buku kepada pengguna dengan hasil yang relevan dan sesuai prediksi.

# **Kesimpulan**

Secara keseluruhan, model rekomendasi buku yang dikembangkan menggunakan pendekatan Content-based Recommendation dan Collaborative Filtering Recommendation telah berhasil diimplementasikan dan dapat memberikan rekomendasi yang sesuai dengan preferensi pengguna. Pada metode Collaborative Filtering, sistem memerlukan data rating dari pengguna untuk menghasilkan rekomendasi, sementara pada Content-based Filtering, data rating tidak diperlukan, karena sistem menganalisis atribut dari setiap buku untuk menentukan kecocokannya dengan preferensi pengguna. Dengan kedua pendekatan ini, sistem dapat memberikan rekomendasi yang lebih personal dan relevan bagi pengguna, berdasarkan interaksi mereka maupun karakteristik buku itu sendiri.
"""